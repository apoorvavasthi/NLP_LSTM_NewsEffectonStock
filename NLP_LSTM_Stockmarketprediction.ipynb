{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OWrN-OJDqG5",
        "outputId": "e5143e9b-f0be-4820-9092-01a43b3adbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "1Vhlh93kJSC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "#porterstemmer is used for stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "import re\n",
        "import pandas as pd\n",
        "import chardet as chardet\n",
        "#Data cleaning & preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#Deep Learning (Embedding & LSTM)\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Embedding,Dense,LSTM\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
        "from tensorflow.data import Dataset\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXIV_L43Bcg4",
        "outputId": "d94b4134-315d-4b67-ab62-35f7e5929e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading CSV"
      ],
      "metadata": {
        "id": "UZgMbv0vJVbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKuoFwprD73D"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/gdrive/MyDrive/Datasets/stockmarket/Combined_News_DJIA.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KLSB2FGFe5z",
        "outputId": "68fd2cee-bd9d-4ebb-a08e-49b59d9ce1b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2008-08-08', '2016-07-01')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.Date.min(),df.Date.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhXmckPOGLGP",
        "outputId": "2ebe9524-0217-42b7-c063-5f0f6ef2842f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date     0\n",
              "Label    0\n",
              "Top1     0\n",
              "Top2     0\n",
              "Top3     0\n",
              "Top4     0\n",
              "Top5     0\n",
              "Top6     0\n",
              "Top7     0\n",
              "Top8     0\n",
              "Top9     0\n",
              "Top10    0\n",
              "Top11    0\n",
              "Top12    0\n",
              "Top13    0\n",
              "Top14    0\n",
              "Top15    0\n",
              "Top16    0\n",
              "Top17    0\n",
              "Top18    0\n",
              "Top19    0\n",
              "Top20    0\n",
              "Top21    0\n",
              "Top22    0\n",
              "Top23    1\n",
              "Top24    3\n",
              "Top25    3\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQsVI3ALIAJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1887cd9-b486-4ea3-8a49-cb097d1cd6d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1986, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.dropna(how='any',axis=0,inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wljmEqw_KW45",
        "outputId": "2d1d8df1-a076-4fe7-a902-404b242f1000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1062\n",
              "0     924\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting into Train, Test and doing Preprocessing"
      ],
      "metadata": {
        "id": "m3DlzbVbJdrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LfqXUA_rc0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b622326-09cd-4787-9613-408e2f46deee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-35a15c00515e>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[col1][row1]=(review)\n",
            "<ipython-input-10-35a15c00515e>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train[\"News\"]=train[text].agg('  '.join, axis=1)\n",
            "<ipython-input-10-35a15c00515e>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test[col2][row2]=(review)\n",
            "<ipython-input-10-35a15c00515e>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test[\"News\"]=test[text].agg('  '.join, axis=1)\n"
          ]
        }
      ],
      "source": [
        "train=df[df[\"Date\"]<'2015-01-01']\n",
        "test=df[df[\"Date\"]>='2015-01-01']\n",
        "\n",
        "text=train.iloc[:,3:].columns\n",
        "#Text cleanning and Preprocessing for Train data\n",
        "\n",
        "for col1 in text:\n",
        "  for row1 in range (0,len(train)):\n",
        "    review=re.sub('[^a-zA-Z]',' ',train[col1][row1])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    #review=[ps.stem(word) for word in review]\n",
        "    review=' '.join(review)\n",
        "    train[col1][row1]=(review)\n",
        "train[\"News\"]=train[text].agg('  '.join, axis=1)\n",
        "X_train=train[['Date','News']]\n",
        "Y_train=train[\"Label\"]\n",
        "#Text cleanning and Preprocessing For Test data\n",
        "for col2 in text:\n",
        "  for row2 in range (1608,test.index.max()):\n",
        "    review=re.sub('[^a-zA-Z]',' ',test[col2][row2])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    #review=[ps.stem(word) for word in review]\n",
        "    review=' '.join(review)\n",
        "    test[col2][row2]=(review)\n",
        "test[\"News\"]=test[text].agg('  '.join, axis=1)\n",
        "X_test=test[['Date','News']]\n",
        "Y_test=test[\"Label\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J47_-xrDEqNa",
        "outputId": "81f5faee-420e-4241-b5e5-90228b878e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1608, 2), (1608,), (378, 2), (378,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unique words count from a column of a dataframe\n",
        "from collections import Counter\n",
        "results = Counter()\n",
        "X_train[\"News\"].str.lower().str.split().apply(results.update)\n",
        "print(len(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atv80vy2LSl2",
        "outputId": "45a4ca19-c9d7-46ae-9587-e85bdb11ec31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text to vector using Embedding layers,but Embedding layers only takes numerical values hence first we need to convert the text to encoded vectors\n",
        "2 ways to do that\n",
        "1. One_hot  \n",
        "2. Text Vectorization  \n",
        "---\n",
        "\n",
        "1 Converting text to vector(Using one_hot encoding)for feeding into Embedding layer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qabi_LHGJ7e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size=2000\n",
        "max_words_list=2500\n",
        "# embed_dim=64\n",
        "encod_corp=[]\n",
        "for i,doc in enumerate(X_train[\"News\"]):\n",
        "    encod_corp.append(one_hot(doc,2000))\n",
        "padded_corp = pad_sequences(encod_corp,maxlen=max_words_list,padding='post')"
      ],
      "metadata": {
        "id": "O6NCFSESxxuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "2: Converting text to vector(Using Text Vectorization)for feeding into Embedding Layer of LSTM\n"
      ],
      "metadata": {
        "id": "3hSeiKQiKpEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=tuple(train[\"News\"])\n",
        "output_sequence_length = 300\n",
        "vocab_size = 2000\n",
        "# Create the TextVectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "                  output_sequence_length=output_sequence_length,\n",
        "                  max_tokens=vocab_size)\n",
        "# Train the layer to create a dictionary\n",
        "vectorize_layer.adapt(sentences)\n",
        "# Convert all sentences to tensors\n",
        "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
        "# Use the word tensors to getvectorized  phrases\n",
        "vectorized_words = vectorize_layer(word_tensors)\n",
        "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
        "print(\"Vectorized words: \", vectorized_words)\n",
        "\n",
        "\n",
        "#Output:\n",
        "'''Vocabulary:  ['', '[UNK]', 'the','targeting', 'slams', 'sharia', 'review', 'raised', 'putting', 'pot', 'planes', 'morning', 'monsanto', 'loss', 'legalization', 'iranians', 'holy', 'holds', 'hidden', 'guy', 'gangs', 'eyes', 'commercial', 'ceo', 'attempted', 'atlantic', 'abbott', 'visa', 'swine', 'surprise', 'sharing', 'sexually', 'sets', 'scientific', 'ring', 'reserves', 'prosecutors', 'programme', 'prepares', 'possibly', 'panel', 'ones', 'lot', 'lord', 'lama', 'km']\n",
        "Vectorized words:  tf.Tensor(\n",
        "[[   7  780    1 ...    2  275    4]\n",
        " [   7  201    1 ...  249    0    0]\n",
        " [   7 1609   13 ...  780    8  295]\n",
        " ...\n",
        " [ 776  170    1 ...  422    1   47]\n",
        " [  36    1  493 ... 1836 1021 1089]\n",
        " [   1  973  103 ...  367    1  252]], shape=(1608, 300), dtype=int64)'''\n",
        "\n",
        "\n",
        "# output_length = 300\n",
        "# word_embedding_layer = Embedding(vocab_size, output_length)\n",
        "# embedded_words = word_embedding_layer(vectorized_words)\n",
        "# print(embedded_words)"
      ],
      "metadata": {
        "id": "qFlLiYnYKoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Deep neural network RNN LSTM\n"
      ],
      "metadata": {
        "id": "Gc0SxvrUN2Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 24\n",
        "lstm_out = 16\n",
        "batch_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(50, embed_dim,input_length = padded_corp.shape[1]))\n",
        "model.add(LSTM(lstm_out,activation='relu',kernel_regularizer=keras.regularizers.l2()))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "optimizers = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer=optimizers,metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(padded_corp,Y_train,epochs=5,verbose=1)"
      ],
      "metadata": {
        "id": "QeAMq8kmNfTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "scores = model.evaluate(padded_corp_1, Y_test,verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "tGkoHtqUJhdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are using ML Models:\n",
        "> Convert text data to Vectors using:\n",
        "\n",
        "1. Bag of words\n",
        "2. TFIDF\n",
        "3. Word2Vec\n",
        "---\n",
        "\n",
        "1)Bag of words"
      ],
      "metadata": {
        "id": "XIPb0-C1OObT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hExfEgiV75UF"
      },
      "outputs": [],
      "source": [
        "# #Creating a bag of words model\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv=CountVectorizer(binary=True,ngram_range=(2,2))\n",
        "# X_train_bow=cv.fit_transform(new_df[\"News\"]).toarray()\n",
        "# X_train_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT8iHx5exiVe"
      },
      "source": [
        "---\n",
        "2)TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8H9cm2dNwVW"
      },
      "outputs": [],
      "source": [
        "# #Creating a tfidf model\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# #max features =2500 ------> it means that out of total max occuring words only top 2500 words will be selected for training the model\n",
        "# tf=TfidfVectorizer(ngram_range=(2,2))\n",
        "# X=tf.fit_transform(new_df[\"News\"]).toarray()\n",
        "#Y_train_bow=new_df['Label']\n",
        "#X_train_bow.shape,Y_train_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Models:\n",
        "\n",
        "\n",
        "---\n",
        "Naive_bayes\n"
      ],
      "metadata": {
        "id": "1Mf80rAVRj6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXbkuzZJz8ci"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "stock_model=MultinomialNB().fit(X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJG2hKR0D9w"
      },
      "outputs": [],
      "source": [
        "y_pred=stock_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7FcBylq0FWF",
        "outputId": "6cd5b62e-a90a-44c8-ebca-45721df1c0df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4973544973544973"
            ]
          },
          "execution_count": 254,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "score=accuracy_score(Y_test,y_pred)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg72U-JG0G7L",
        "outputId": "850b586a-e202-4b09-d10b-ab564708a09b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.45      0.17        44\n",
            "           1       0.88      0.50      0.64       334\n",
            "\n",
            "    accuracy                           0.50       378\n",
            "   macro avg       0.49      0.48      0.41       378\n",
            "weighted avg       0.79      0.50      0.58       378\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred,Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForest"
      ],
      "metadata": {
        "id": "AjxjQEujSKiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2KcwCHd0TXj"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\n",
        "classifier.fit(X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFLAVUMD0TZv"
      },
      "outputs": [],
      "source": [
        "y_pred_rdf=classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Z9b3NY0XdL"
      },
      "outputs": [],
      "source": [
        "score=accuracy_score(Y_test,y_pred_rdf)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJmCJEk0ZIz",
        "outputId": "12dbee32-873a-443c-f8c9-b541accf873a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.40      0.04        10\n",
            "           1       0.97      0.51      0.66       368\n",
            "\n",
            "    accuracy                           0.50       378\n",
            "   macro avg       0.50      0.45      0.35       378\n",
            "weighted avg       0.94      0.50      0.65       378\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred_rdf,Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All In One ML Models"
      ],
      "metadata": {
        "id": "3iKFHnN1SA4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  models = dict()\n",
        "  #models['XGB'] = XGBClassifier()\n",
        "  #models['LGBM'] = LGBMClassifier()\n",
        "  models['lr'] = LogisticRegression()\n",
        "  models['knn'] = KNeighborsClassifier()\n",
        "  models['cart']= DecisionTreeClassifier()\n",
        "  models['rnf'] = RandomForestClassifier()\n",
        "  models['svm'] = SVC(probability=True)\n",
        "  models['bayes'] = GaussianNB()\n",
        "  models['SGD'] = SGDClassifier()\n",
        "\n",
        "  return models\n",
        "\n",
        "def model_predict(x,y,model,xt):\n",
        "  model.fit(x,y)\n",
        "  Y_pre = model.predict(xt)\n",
        "  # Y_pred_prob = model.predict_proba(X_test_1)\n",
        "  return Y_pre\n",
        "\n",
        "def model_pre_pro(x,y,model,xt):\n",
        "  model.fit(x,y)\n",
        "  Y_pred_prob = model.predict_proba(xt)\n",
        "  return Y_pre\n",
        "\n",
        "models = get_models()\n",
        "\n",
        "\n",
        "Score = []\n",
        "Algo =[]\n",
        "for name, model in models.items():\n",
        "    Y_pre = model_predict(X_train_1,Y_train,model,X_test_1)\n",
        "    Score_DT = accuracy_score(Y_test,Y_pre)\n",
        "    #F1 = f1_score(Y_test,Y_pre)\n",
        "    #print('>%s % .3f'% (name,Score_DT))\n",
        "    Algo.append(name)\n",
        "    Score.append(Score_DT )\n",
        "\n",
        "Classification_result = pd.DataFrame(list(zip(Algo,Score)), columns=['Alo_name','Accuracy'])\n",
        "Classification_result\n"
      ],
      "metadata": {
        "id": "GAo_J8g2riIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}